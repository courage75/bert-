{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "import pandas as pd\n",
    "df=pd.read_excel(\"weibo.xlsx\")\n",
    "\n",
    "df=df.replace(1,2)\n",
    "df=df.replace(0,1)\n",
    "df=df.replace(-1,0)\n",
    "comments=df['comment'].tolist()\n",
    "labels=df['label'].tolist()\n",
    "length=len(df)\n",
    "split_num=int(length*0.7)\n",
    "train_data=df[:split_num]\n",
    "test_data=df[split_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at D:\\code\\model\\bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "\n",
    "# 1. 加载预训练的tokenizer和模型\n",
    "tokenizer = BertTokenizer.from_pretrained('D:\\\\code\\\\model\\\\bert-base-chinese')\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"D:\\\\code\\\\model\\\\bert-base-chinese\", # 使用12层的BERT模型\n",
    "    num_labels = 3, # 二分类任务（比如情感分析）\n",
    "    output_attentions = False, # 模型是否返回注意力权重\n",
    "    output_hidden_states = False, # 模型是否返回所有隐藏状态\n",
    ")\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=model.to(device)\n",
    "# 2. 准备数据\n",
    "# 假设我们有一些文本数据和对应的标签\n",
    "texts = train_data['comment'].tolist()\n",
    "texts=[str(text)for text in texts]\n",
    "labels = train_data['label'].tolist()  # 1代表积极情绪，0代表消极情绪\n",
    "# 使用tokenizer处理文本数据\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "# 把标签转换成Tensor\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# 3. 创建一个DataLoader\n",
    "data = list(zip(inputs['input_ids'], inputs['attention_mask'], labels))\n",
    "dataloader = DataLoader(data, batch_size=2)\n",
    "\n",
    "# 4. 微调模型\n",
    "# 设置优化器\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# 开始训练\n",
    "model.train()\n",
    "for epoch in range(3):  # 这里只做3个epoch的训练\n",
    "    for batch in dataloader:\n",
    "        input_ids,attention_mask,labels = batch\n",
    "        input_ids=input_ids.to(device)\n",
    "        attention_mask=attention_mask.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# 5. 保存微调后的模型\n",
    "torch.save(model, 'complete_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对已经训好的模型做测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hjg\\AppData\\Local\\Temp\\ipykernel_2804\\3821739227.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model=torch.load(\"D:\\\\code\\\\情感分析\\\\complete_model.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model=torch.load(\"D:\\\\code\\\\情感分析\\\\complete_model.pth\")\n",
    "model.eval()\n",
    "test=[\"I love the book\",\"I hate the book\"]\n",
    "tokrnizer=BertTokenizer.from_pretrained('D:\\\\code\\\\model\\\\bert-base-uncased')\n",
    "test=tokenizer(test,padding=True,truncation=True,max_length=128,return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions=model(**test)\n",
    "\n",
    "logits=predictions.logits\n",
    "_,pre_label=torch.max(logits,1)\n",
    "# 保存预测的标签\n",
    "predicted_labels_list = pre_label.tolist()\n",
    "# ...保存到文件或其他操作\n",
    "predicted_labels_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
